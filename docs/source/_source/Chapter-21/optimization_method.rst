=================
最优化方法
=================


拟牛顿算法
=================

牛顿法
-----------------


牛顿法（经典牛顿法）的迭代表达式：

    .. math::

        x^{k+1} = x^k  - \nabla^2 f(x^k)^{-1} \nabla f(x^k)

但是，牛顿法过程中 Hessian 矩阵 :math:`\nabla^2 f(x^k)^{-1}` 的计算和存储的代价很高，对于条件数较多的问题很难求解。因此，引入 **拟牛顿法**。

拟牛顿法
-----------------

**拟牛顿法** 的核心思路在于，在牛顿法的迭代过程中，用 **近似解** 计算第 :math:`k` 次迭代下的 Hessian 矩阵 :math:`\nabla^2 f(x^k)` ，近似值记为 :math:`B^k` ，即有 :math:`B^k \approx \nabla^2 f(x^k)` ，称为 **拟牛顿矩阵**。

用 **近似值** :math:`B^k` 代替牛顿法中的 :math:`\nabla^2 f(x^k)`，得：

    .. math::

        x^{k+1} = x^k - B(x^k)^{-1} \nabla f(x^k)

在近似 Hessian 矩阵时，也需要通过 **某种映射关系** 并 **不断迭代** 得到。但是依然需要求近似矩阵的逆，为了避免计算逆矩阵的开销，我们可以 **直接近似** Hessian **矩阵的逆**，记 :math:`H^k = (B^k)^{-1}` 。故我们有：

    .. math::

        x^{k+1} = x^k - H^k\nabla f(x^k)  \\

        H^{k+1} = g(H^k)

其中 :math:`g` 为 **近似** Hessian **矩阵的逆** 的映射函数。一般有 :math:`H^{k+1} = H^k +C^k`，其中 :math:`C^k` 被称为 **修正矩阵**。

拟牛顿法基本过程
--------------------

* 令 :math:`H^0 = I`，任选初始点 :math:`x^0 \in \mathbb {R}^n`，令:math:`k = 0`
* 计算 **梯度** :math:`\nabla f(x^k)` ，如果满足终止条件 :math:`|| \nabla f(x^k)|| \lt \epsilon`，取 :math:`x^{*} = x^k`，并结束整个算法
* 计算 **搜索方向** :math:`d^k = -H^k \nabla f(x^k)` ， :math:`H^k` 为当前 :math:`x^k` 处的Hessian 矩阵的近似
* 迭代更新 :math:`x: x^{k+1} = x^{k} + d^k`
* 更新 :math:`H: H^{k+1} = g(H^k)` 根据 :math:`x^k` 点的信息进行简单修正